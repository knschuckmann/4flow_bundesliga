# -*- coding: utf-8 -*-
"""4flow_challenge.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/10a_IGibcAmmU5HAQtAlawpGpl_2miSsE

# 4flow Challenge

We want to welcome everybody to the annual 4flow Challenge in Berlin.


### Task
You will split up in teams of x people and solve the given Excercise under our Instruction. For this purpose, we require:
- Laptop
- Google Drive account
- Concentration
- Teamwork

### Goal
To understand some minor machine learning algorithms through exercises and examine the possibilities of machine learning.


## Excercise
You will receive a dataset on Bundesliga results. Your task will be to display the dataset but moreover to predict the outcome of some matches.

⛔️  you will need to copy this Notebook to your own Google drive account

1. Click on File in Menu 
2. Click on 'Save a copy in Drive' as shown below <br/>
![copy File][2] 

3. A new Tab will be created, so you can start running this script.

[2]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Save.PNG

## Data and Library load

### ⚠️ How Colab works

1. Colab is an online [Jupyter Notebook](https://jupyter.org/index.html), which can be accessed without any further installation
2. By Connecting to a computation power source a new instance will be created. For now we will devote ourselves, that it works and not dive into details. <br/>
![runtime][2]
3. Clicking on the play Button in each Codefield, runs the code inside this Codefield. <br/>
![run_o][3] <br/>
Only if you hover on the area of the button you will be able to see the play button<br/> ![run_1][4]
4. After you ran a cell you will see a number on the field, marking the order of the running  process  <br/>
![run_1][5]
4. # ⚠️ **Possible Errors**  
  1. It is essential to follow the order of this Notebook and **run the Codefields one after the other**. the numbers will tell you if you forgot to run a field
  2. Make sure you **copy and paste right**, without any white spaces. Especially in the feature selection part, it will be important to avoid this error 

[2]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Runtime.PNG
[3]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Run_0.PNG
[4]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Run_1.PNG
[5]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Run_2.PNG

Start importing libraries and dataset by running every Code field sequentially
"""

# Import necessary libraries
import pandas as pd
import numpy as np
import plotly.graph_objects as go
from sklearn import preprocessing
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.model_selection import GridSearchCV

# declare static variables (urls)
url_orig = 'https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/Bundesliga_Data_for_2018_2019.csv'
url_goals = 'https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/goals.csv'

"""Here you will be able to see the output of the last line from the Code field. It will display the first four lines of the dataset, so that you can take a quick look at the data. 
#####Display
"""

# Load the dataset from Github link
data = pd.read_csv(url_orig, delimiter=';', decimal=',')
goals = pd.read_csv(url_goals, delimiter=';', decimal=',')
# merge both CSV files to one dataset
data = pd.merge(data, goals, on ='KEY')
data.head()

"""## Understanding the dataset
It is essential to understand the dataset and get all the necessary insights about it.

What do the column headings mean? It would help if you described them one by one.

**The following table explains the column headings**
<br>
<br>

<table>
  <tr>
    <th>div</th>
    <th>Date</th>
    <th>HomeTeam</th>
    <th>AwayTeam</th>
    <th>KEY</th>
    <th>Home_Team Size</th>
    <th>Home_Market Value total</th>
    <th>Home_Market Value avg</th>
    <th>Away_Team Size</th>
    <th>Away Market Value total</th>
    <th>Away_Market Value avg</th>
    <th>HS (Home Team Shots) </th>
    <th>AS (Away Team Shots)</th>
    <th>HAST (Home Team Shots on Target)</th>
    <th>AST (Away Team Shots on Target)</th>
    <th>HF (Home Team Fouls Committed)</th>
    <th>AF (Away Team Fouls Committed)</th>
    <th>HC (Home Team Corners)</th>
    <th>AC (Away Team Corners)</th>
    <th>HY (Home Team Yellow Cards)</th>
    <th>AY (Away Team Yellow Cards)</th>
    <th>HR (Home Team Red Cards)</th>
    <th>AR (Away Team Red Cards)</th>
    <th>Home #Win</th>
    <th>Home #Draw</th>
    <th>Home #Defeat</th>
    <th>Away #Win</th>
    <th>Away #Draw</th>
    <th>Away #Defeat</th>
    <th>Home goals scored</th>
    <th>Home goals collected</th>
    <th>Away goals scored</th>
    <th>Away goals collected</th>
    <th>Home Position in the table</th>
    <th>Away Position in the table</th>
    <th>FTHG (Full Time Home Team Goals)</th>
    <th>FTAG (Full Time Away Team Goals)</th>
    <th>FTR (Full Time Result)</th>
    <th>HTHG (Half Time Home Team Goals)</th>
    <th>HTAG (Half Time Away Team Goals)</th>
    <th>HTR (Half Time Goals)</th>
  </tr>
  <tr>
    <td>String: No definition</td>
    <td>String: date of the game in dd.mm.yyyy format</td>
    <td>String: Name of Home team</td>
    <td>String: Name of Away team</td>
    <td>String: Combined date_HomeTeam_AwayTeam unique key</td>
    <td>integer: Size of Home team</td>
    <td>Float: Market value of Home Team in million €</td>
    <td>Float: Market average value of Home Team in million</td>
    <td>Integer: Away team size</td>
    <td>Float: Market value of Away Team in million €</td>
    <td>Float: Market average value of Away Team in million</td>
    <td>Integer: Total shots scored by Home Team in season</td>
    <td>Integer: Total shots scored by Away Team in season</td>
    <td>Integer: Total shots on goal scored by Home Team in season</td>
    <td>Integer: Total shots on goal scored by Away Team in season</td>
    <td>Integer: Total Fouls commited by Home Team in season</td>
    <td>Integer: Total Fouls commited by Away Team in season</td>
    <td>Integer: Total shots from corners by Home Team in season</td>
    <td>Integer: Total shots from corners by Away Team in season</td>
    <td>Integer: Total received yellow cards by Home Team in season</td>
    <td>Integer: Total received yellow cards by Away Team in season</td>
    <td>Integer: Total received red cards by Home Team in season</td>
    <td>Integer: Total received red cards by Away Team in season</td>
    <td>Integer: Total wins by Home Team in season</td>
    <td>Integer: Total draws by Home Team in season</td>
    <td>Integer: Total loosings by Home Team in season</td>
    <td>Integer: Total wins by Away Team in season</td>
    <td>Integer: Total draws by Away Team in season</td>
    <td>Integer: Total loosings by Away Team in season</td>
    <td>Integer: Total goals scored by Home Team in season</td>
    <td>Integer: For Home Team total goals received in season</td>
    <td>Integer: Total goals scored by Away Team in season</td>
    <td>Integer: For Away Team total goals received in season</td>
    <td>Integer: Home position in the table</td>
    <td>Integer: Away position in the table</td>
    <td>Integer: Home Team goals scored during match</td>
    <td>Integer: Away Team goals scored during match</td>
    <td>String: Home win/ Away Win/ Draw after match</td>
    <td>Integer: Home Team goals scored until Half Time</td>
    <td>Integer: Away Team goals scored until Half Time</td>
    <td>String: Home win/ Away Win/ Draw until Half Time</td>
  </tr>
</table>

We made some minor analysis of the dataset. The following images show some insights into the dataset. This is helpful in order to understand the data.

The Overview page shows the overall picture of the dataset: How many variables, observations, etc. </br>
![Overview][1]

The crucial part is the displayed warnings section </br>
![Warnings][2] </br>
where you not only get the high cardinality, which could be used as the independent variable but more than that, you get correlation relationships between each feature. 

Now you can omit some high correlated columns or some important warnings by cleansing the dataset.


[1]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Overview.PNG
[2]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Warnings.PNG

### Data cleansing

1. Convert the date format from String to date format (Computer needs Date format and not Date in String format, compare with previous table print)
2. Because some columns do not contain any information, we like to have them dropped out. One good example is the Div column, which consists only out of a constant. 
3. We also have a lot of correlated columns in this dataset. Sometimes it is redundant information. Especially when dealing with regression problems, highly correlated data should be omitted. We will do so for just a few columns, so you get the idea of it.
"""

# 1 dateformat
data.Date = pd.to_datetime(data.Date, format= '%d.%m.%Y')
# 2 drop Div column
data = data.drop(['Div'], axis = 1)
# 3 drop correlated
data = data.drop(['Home_Market Value avg', 'Away_Market Value avg', 'HAST (Home Team Shots on Target)', 'AST (Away Team Shots on Target)','HC (Home Team Corners)', 'AC (Away Team Corners)'], axis = 1)

# minor preprocessing requirements like sort data
data = data.sort_values(by= 'Date')
data = data.reset_index(drop=True)

data.head()

data.loc[175]

"""We still have some categorical values that we can use to transform into numerical, so we will be able to plot easier. At the very end of the printed image, you will find the created dummies and understand what we did."""

# create dummy variable for Full Time Results and half Time Results 
data_dummies = pd.get_dummies(data, columns=['FTR (Full Time Result)'], prefix= 'FTR')
data_dummies = pd.get_dummies(data_dummies, columns=['HTR (Half Time Result)'], prefix= 'HTR')
data_dummies.head()

"""Now that we have cleaned our data a bit, we can start exploring the dataset by plotting it. This requires some further data preparation. Here we will prepare a set so we can obtain the number of winnings or the overall sum of goals."""

# create empty data frames
data_goals = pd.DataFrame()
data_win = pd.DataFrame()
teams = data_dummies['HomeTeam'].unique()
# fill empty dataframes with winning information and goals amount
for team in teams:
  data_win = data_win.append(data_dummies[data_dummies['HomeTeam'] == team][['HomeTeam','Home #Win', 'Home #Draw', 'Home #Defeat']].tail(1))
  data_goals = data_goals.append(data_dummies[data_dummies['HomeTeam'] == team][['HomeTeam','Home goals scored','Home goals collected']].tail(1))
data_win = data_win.reset_index(drop = True)
data_goals = data_goals.reset_index(drop = True)

"""Here you can take a quick look at the datasets 
1. Wins 
2. Goals
"""

data_win.head()

data_goals.head()

"""All preparation is done, and we can start to display the dataset. The goal is to find some insights through the plots.

We use a very nice and straightforward library called plotly, which allows us to play around and hide some variables if required.

You will find an interactive legend on the right side of the plot, and you can play around by clicking on the legends headings. If we do so, we will find a correlation of Home Team Goals with the factor of how many times the Team won a match.
"""

# Create traces
fig = go.Figure()
fig.add_trace(go.Scatter(x=data_win.HomeTeam.unique(), y=data_win['Home #Win'],
                    mode='lines',
                    name= 'Home_Win_amount'))
fig.add_trace(go.Scatter(x=data_win.HomeTeam.unique(), y=data_win['Home #Draw'],
                    mode='lines',
                    name= 'Home_Draw_amount'))
fig.add_trace(go.Scatter(x=data_win.HomeTeam.unique(), y=data_win['Home #Defeat'],
                    mode='lines',
                    name= 'Home_Defeat_amount'))
fig.add_trace(go.Scatter(x=data_goals.HomeTeam.unique(), y=data_goals['Home goals scored'],
                    mode='lines',
                    name= 'Home_Goals_scored'))
fig.add_trace(go.Scatter(x=data_goals.HomeTeam.unique(), y=data_goals['Home goals collected'],
                    mode='lines',
                    name= 'Home_Goals_collected'))
fig.update_layout(yaxis_title="total values")
fig.show()

"""The output of the plot above shows the actual amount (a number) of either winning, draw or defeat of one Team compared to each other as well as the goals scored and collected.

Sometimes you have data that can not be compared to each other because of the scale. So you could scale everything into a normal distribution with the scale function from the library Sklearn. This normalizes any dataset to the standard deviation $N(\mu,\sigma^2)$ with mean equal to zero and variance equal to one.

What we will see is that the average value of teams is correlated to the total value, which is not really impressive, but in order to compare datasets, it is a useful realization.
"""

fig = go.Figure()
fig.add_trace(go.Scatter(x=data_dummies.HomeTeam.unique(), y=preprocessing.scale(data_dummies['Home_Team Size']),
                    mode='lines + markers',
                    name= 'scaled team size'))
fig.add_trace(go.Scatter(x=data_dummies.HomeTeam.unique(), y=preprocessing.scale(data_dummies['Home_Market Value total']),
                    mode='lines + markers',
                    name= 'scaled total value'))
fig.add_trace(go.Scatter(x=data_win.HomeTeam, y=preprocessing.scale(data_win['Home #Win']),
                    mode='markers',
                    name= 'scaled Home win amount'))
fig.update_layout(yaxis_title="scaled values")
fig.show()

"""The figure above also shows some fascinating insights. Especially if you take a look at FC Bayern München, you will see that the proportion of the team size and the team value is completely different than for all other teams. Nevertheless, FC Bayern München won many matches. Borussia Dortmund is another team that has a similar proportion to FC Bayern München.

## Machine learning Algorithms

There are many algorithms out there, but mostly you will be working with a hand full of them. 

1. First, you need to make sure what to predict. In order to do so, one needs a good question or a statement to start with. **Predict a result of a certain game!**

2. Secondly, you will need to decide if you want to predict a category or a numerical number. This can be done by looking at the variable you want to predict. In our case, it is a  classification problem. Therefore we will concentrate on classification algorithms.

3. We need to separate our data into a training and a test set.
  - The training set will be used to train the model 
  - The test set will be used to validate the model and get the goodness of our trained model

**So let's start predicting for real now...**
"""

# calculate the number of testing values
size_test = int(data.shape[0]*0.2)

# create dummy data for better representation of original data
dummy_data = pd.get_dummies(data, columns=['HomeTeam'], prefix= 'Home')
dummy_data = pd.get_dummies(dummy_data, columns=['AwayTeam'], prefix= 'Away')

# get rid of categorical data 
dummy_data = dummy_data[dummy_data.columns.difference(['HTR (Half Time Result)', 'FTR (Full Time Result)', 'Date', 'KEY'])]

"""**Goal:**</br>
The goal is to find the best model by choosing different features from the following displayed once  
1. Choose a maximum of **12 features** for further calculation and prediction
  * You will be able to repeat this step over and over until you find the best result 

Example of some features:</br>
'AF (Away Team Fouls Committed)', 'AR (Away Team Red Cards)', 'AS (Away Team Shots)', 'AY (Away Team Yellow Cards)', 'Away #Defeat', 'Away #Draw', 'Away #Win', ...

Following all features by running the code:
"""

dummy_data.columns

"""Lets play around and find the best features for our prediction. 

⚠️ Copy some features above and put them into the brackets below dummy_data[[ 'feature1', 'feature2', 'feature3' ]]. </br>
Try to choose wisely and make sure to **copy-paste right**</br>

⛔️ After you get the result, you will be able to start over from here with different features to improve your model. Just choose different features and run the cells again.

## Loop start
"""

# choose features and insert and cretae a new dataframe with the chosen features  
dummy_data_1 = dummy_data[[" delete this and copy features from above like 'Home_Werder Bremen' and separate by comma "]]

# initialize the test and training data
X_train = dummy_data_1[:-size_test]
X_test = dummy_data_1[-size_test:]

"""⚠️ The mapping of the dependent variable y is as follows,
* 2 = Home Win
* 1 = Draw
*  0 = Away Win
"""

# code the prediction variable and initialize the prediction set
# 2 = Home Win, 1 = Draw, 0 = Away Win
y = data['FTR (Full Time Result)']
y = y.astype('category').cat.codes
y_train = y[:-size_test]
y_test = y[-size_test:]

"""Here we begin to predict the outcome of any given match. Therefore we need to use some Machine learning algorithms. The most common once we use in the upcoming prediction are:


1.   **[Logistic Regression][log]**
2.   **[Support Vector Machines][SVM]**
3.   **[Random Forrest][RF]**

Those are state of the art for classification problems. Later on, we will try to improve our results. The displayed table gives a good insight into the prediction. You will have the original result on the very right and the predictions on the left. </br>
Remember: </br>
* 0 = Away Win
* 1 = Draw
* 2 = Home Win
[log]: https://www.empirical-methods.hslu.ch/entscheidbaum/zusammenhaenge/logistische-regression/
[SVM]: https://towardsdatascience.com/support-vector-machine-introduction-to-machine-learning-algorithms-934a444fca47
[RF]: https://towardsdatascience.com/an-implementation-and-explanation-of-the-random-forest-in-python-77bf308a9b76
"""

# Logistic Regression for classification 
lr = LogisticRegression(random_state=0, max_iter= 10000).fit(X_train, y_train) 
result_lr = lr.predict(X_test)

# Support Vector Machines for classification 
svm = SVC(random_state=0).fit(X_train, y_train)
result_svm = svm.predict(X_test)

# Randoom Forrst for classification 
rf = RandomForestClassifier(random_state = 0).fit(X_train,y_train)
result_rf = rf.predict(X_test)

# concat all results together with the original results
result_lr = pd.DataFrame(result_lr, columns= ['pred_lr'])
result_svm = pd.DataFrame(result_svm, columns= ['pred_svm'])
result_rf = pd.DataFrame(result_rf, columns= ['pred_rf'])
result_org = pd.DataFrame(y_test.values,  columns= ['orig'])

result = pd.concat([result_lr, result_svm, result_rf, result_org],1)
result

"""Let's plot the results so we can actually get the predictions and grasp them. 
We will plot the differences between the algorithm predictions and the original result as well as the original result. </br>
Formula: </br>
$|\text{predicted} - \text{original}|$

**Differences:**
* 0 := prediction same as the original result
* 1, 2 := wrong prediction
"""

fig = go.Figure()
fig.add_trace(go.Scatter(x=abs(result.pred_lr - result.orig), y= data.KEY[-size_test:],
                    mode='markers',
                    name= 'difference_lr_orig'))
fig.add_trace(go.Scatter(x=abs(result.pred_svm - result.orig), y= data.KEY[-size_test:],
                    mode='markers',
                    name= 'difference_svm_orig'))
fig.add_trace(go.Scatter(x=abs(result.pred_rf - result.orig), y= data.KEY[-size_test:],
                    mode='markers',
                    name= 'difference_rf_orig'))
fig.add_trace(go.Scatter(x= result.orig , y=data.KEY[-size_test:],
                    mode='markers',
                    name= 'original'))
fig.show()

"""The Table and the Plot above are already good indicators for the goodness, but the actual scores, calculated by a simple rule of thumb where you divide right predicted by the actual number of sample, is a better indicator for the goodness of the algorithm.

**For the best result, try to improve only one of the following 3 values** </br>
⚠️ Following, the prediction results:
"""

temp = {'lr': [lr.score(X_test, y_test)], 
        'svm': [svm.score(X_test, y_test)], 
        'rf': [rf.score(X_test, y_test)]
        }

old_scores = pd.DataFrame(temp)
old_scores

"""## Loop end 

If you found the best result, make sure that it is reproducible by running the loop one last time and getting the same results as before. </br>

Now download the .ipynb file and send it to us, by clicking on Download .ipynb</br>
![download][1] 


[1]: https://raw.githubusercontent.com/knschuckmann/4flow_bundesliga/master/pictures/Download.PNG
"""
